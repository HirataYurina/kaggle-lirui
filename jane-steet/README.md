## Market Prediction

### 1. Feature Engineering

#### 1. Miss Data

* 如果特征满足高斯分布，使用mean代替缺失值
* 如果特征呈现偏态分布，使用中位数代替缺失值
* 如果特征是离散值，可以使用众数

#### 2. Outlier

如果发现离群点，可以剔除离群点，不参与模型训练

例如jane street竞赛中的前85天交易频率明显过高，可以直接删除前85天的交易数据

---

### 2. Gradient Boosting

GB算法原理：

参数空间转换到函数空间。

**即把函数f(x)整体看做为损失函数L(y, f(x))的参数θ，这样就可以使用梯度下降法迭代求f(x)的最优解。**

GBDT：Gradient Boosting Decision Tress

利用决策树作为基本学习算法，去学习f(x)。

如果损失函数是L2损失函数，则learner的梯度为f(x) - y，然后再利decision tree去学习残差δ。

#### 3. 决策树和NN

众所周知，决策树的优点有：

* 优秀的可解释性
* 对缺失值不敏感
* 对离群点不敏感
* 甚至不需要对inputs进行归一化
* 精度没那个高（使用集成boost算法，提升精度）

神经网络肯定对缺失值是敏感的，因为神经网络的每个batch的shape一定是一致的。

但是，毫无疑问，现在的神经网络绝对是**有效容量最大**，**拟合能力最强**的机器学习算法。

决策树的缺点也很明显：

**容易过拟合**。

所以，需要使用相应的正则化方法。

